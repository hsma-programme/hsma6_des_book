{
  "hash": "3760f00d96a0fb378d9584dfb8a4e8cc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nauthor:\n  - name: Amy Heather\n    orcid: 0000-0002-6596-3479\n    url: https://github.com/amyheather\nexecute:\n  eval: false\ncode-annotations: hover\n---\n\n# Tests {#sec-tests}\n\n:::{.callout-note title=\"Acknowledgements\"}\n\nThis section is based on the tests in [github.com/pythonhealthdatascience/pydesrap_mms](https://github.com/pythonhealthdatascience/pydesrap_mms) developed by [Amy Heather](https://github.com/amyheather) [![ORCID ID](images/orcid.png)](https://orcid.org/0000-0002-6596-3479) and [Tom Monks](https://github.com/TomMonks) [![ORCID ID](images/orcid.png)](https://orcid.org/0000-0003-2631-4481) from the PenCHORD team at the University of Exeter. This is a SimPy DES model run as part of a reproducible analytical pipeline, and the model structure was based on this book, among other sources.\n:::\n\nTesting is the process of evaluating a model to ensure it works as expected, gives reliable results, and can handle different conditions. By **systematically checking for errors, inconsistencies, or unexpected behaviors**, testing helps improve the quality of a model, catch errors and prevent future issues.\n\n## Pytest\n\nWhen you create a model, you will naturally carry out tests, with simple manual checks where you observe outputs and ensure they look right. These checks can be formalised and **automated** so that you can run them after any changes, and catch any issues that arise.\n\nA popular framework for testing in python is **pytest**.\n\n::: {#91779b04 .cell execution_count=1}\n``` {.python .cell-code}\nimport pytest\n```\n:::\n\n\n![Pytest. Holger Krekel, CC BY 2.5 <https://creativecommons.org/licenses/by/2.5>, via Wikimedia Commons.](images/pytest_logo.png){width=50%}\n\n### Simple pytest example\n\nEach test in pytest is a function that contains an assertion statement to check a condition (e.g. `number > 0`). If the condition fails, pytest will return an error message (e.g. \"The number should be positive\").\n\nTests are typically stored in a folder called `tests`, with filenames starting with the prefix `test_`. This naming convention allows pytest to automatically discover and run all the tests in the folder.\n\nHereâ€™s an example of a simple test using pytest:\n\n::: {#128c518f .cell execution_count=2}\n``` {.python .cell-code}\ndef test_positive():\n    \"\"\"\n    Confirm that the number is positive.\n    \"\"\"\n    number = 5\n    assert number > 0, \"The number should be positive\"\n```\n:::\n\n\n### Running the tests\n\nTests are typically run from the terminal. Commands include:\n\n* `pytest` - runs all tests.\n* `pytest tests/test_example_simple.py` - runs tests from a specific file.\n\nWhen you run a test, you'll see an output like this in the terminal:\n\n:::{.callout-note icon=false}\n\n## Test output:\n\n::: {#bcb5ff33 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n============================= test session starts =============================\nplatform win32 -- Python 3.11.4, pytest-9.0.2, pluggy-1.6.0\nrootdir: C:\\hsma6_des_book\nplugins: anyio-4.8.0\ncollected 1 item\n\ntests\\test_example_simple.py .                                           [100%]\n\n============================== 1 passed in 0.06s ==============================\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n<ExitCode.OK: 0>\n```\n:::\n:::\n\n\n:::\n\n\n### Parametrise\n\nWe can execute the same test on different parameters using `pytest.mark.parametrize`.\n\nHere's an example:\n\n::: {#28820174 .cell execution_count=4}\n``` {.python .cell-code}\n@pytest.mark.parametrize(\"number\", [1, 2, 3, -1])\ndef test_positive_param(number):\n    \"\"\"\n    Confirm that the number is positive.\n\n    Arguments:\n        number (float):\n            Number to check.\n    \"\"\"\n    assert number > 0, f\"The number {number} is not positive.\"\n```\n:::\n\n\nIn this example, we're testing the same logic with four different values: `1`, `2`, `3`, and `-1`. The last value, `-1`, will cause the test to fail. The error message includes the failed value for easy debugging.\n\n:::{.callout-note icon=false}\n\n## Test output:\n\n::: {#7c26e5d4 .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\n============================= test session starts =============================\nplatform win32 -- Python 3.11.4, pytest-9.0.2, pluggy-1.6.0\nrootdir: C:\\hsma6_des_book\nplugins: anyio-4.8.0\ncollected 4 items\n\ntests\\test_example_param.py ...F                                         [100%]\n\n================================== FAILURES ===================================\n___________________________ test_positive_param[-1] ___________________________\n\nnumber = -1\n\n    @pytest.mark.parametrize(\"number\", [1, 2, 3, -1])\n    def test_positive_param(number):\n        \"\"\"\n        Confirm that the number is positive.\n    \n        Arguments:\n            number (float):\n                Number to check.\n        \"\"\"\n>       assert number > 0, f\"The number {number} is not positive.\"\nE       AssertionError: The number -1 is not positive.\nE       assert -1 > 0\n\ntests\\test_example_param.py:13: AssertionError\n=========================== short test summary info ===========================\nFAILED tests/test_example_param.py::test_positive_param[-1] - AssertionError:...\n========================= 1 failed, 3 passed in 0.20s =========================\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<ExitCode.TESTS_FAILED: 1>\n```\n:::\n:::\n\n\n:::\n\n## Coding the model\n\n:::{.callout-tip}\nThroughout the code, anything new that's been added will be followed by the comment `##NEW` - so look out for that in the following code chunks.\n:::\n\nWe will design tests for the model from @sec-reproducibility. However, we will modify the model so that, instead of modifying a global class of parameter values, we **create instances of this class** and use it in our model.\n\nBy using class instances, each test has **isolated parameters**, preventing interference and ensuring consistency. This improves flexibility for independent test scenarios, simplifies debugging, and supports parallel execution by avoiding shared state, making the model more robust.\n\n### Param class\n\nAs these are no longer \"global\" parameters, we will rename `g` to `Param`.\n\n::: {#d4a9ca34 .cell execution_count=6}\n``` {.python .cell-code}\n# Class to store parameter values.\nclass Param:  ##NEW # <1>\n    patient_inter = 5\n    mean_reception_time = 2\n    mean_n_consult_time = 6\n    mean_d_consult_time = 20\n    number_of_receptionists = 1\n    number_of_nurses = 1\n    number_of_doctors = 2\n    prob_seeing_doctor = 0.6\n    sim_duration = 600\n    number_of_runs = 100\n```\n:::\n\n\n1. Renamed `g` to `Param`.\n\n### Patient class\n\nThis remains unchanged.\n\n### Model class\n\nSet parameters as an input to the class. Each instance of `g` is changed to `param` (which refers to the parameter instance provided to the class).\n\n::: {#de9f6773 .cell execution_count=7}\n``` {.python .cell-code}\nclass Model:\n    def __init__(self, param, run_number):  ##NEW # <1>\n        self.param = param  ##NEW # <1>\n        self.env = simpy.Environment()\n        self.patient_counter = 0\n        self.receptionist = simpy.Resource(\n            self.env, capacity=self.param.number_of_receptionists)  ##NEW # <2>\n        self.nurse = simpy.Resource(\n            self.env, capacity=self.param.number_of_nurses)  ##NEW # <2>\n        self.doctor = simpy.Resource(\n            self.env, capacity=self.param.number_of_doctors)  ##NEW # <2>\n\n        ...\n\n        ss = np.random.SeedSequence(self.run_number)\n        seeds = ss.spawn(4)\n        self.patient_inter_arrival_dist = Exponential(\n            mean=self.param.patient_inter,  ##NEW # <2>\n            random_seed=seeds[0])\n        self.patient_reception_time_dist = Exponential(\n            mean=self.param.mean_reception_time,  ##NEW # <2>\n            random_seed=seeds[1])\n        self.nurse_consult_time_dist = Exponential(\n            mean=self.param.mean_n_consult_time,  ##NEW # <2>\n            random_seed=seeds[2])\n        self.doctor_consult_time_dist = Exponential(\n            mean=self.param.mean_d_consult_time,  ##NEW # <2>\n            random_seed=seeds[3])\n\n   ...\n\n    def attend_clinic(self, patient):\n\n        ...\n\n        if random.uniform(0,1) < self.param.prob_seeing_doctor:  ##NEW # <2>\n\n    ...\n\n    def run(self):\n        self.env.process(self.generator_patient_arrivals())\n        self.env.run(until=self.param.sim_duration)  ##NEW # <2>\n\n        ...\n```\n:::\n\n\n1. Set `param` as an input to the `Model`, and made a model attribute.\n2. Replaced all `g` with `self.param`.\n\n### Trial class\n\nSet parameters as an input to the class, and renamed `g` to `param`. Also, disabled printing sections.\n\n::: {#758a990c .cell execution_count=8}\n``` {.python .cell-code}\nclass Trial:\n    def  __init__(self, param):  ##NEW # <1>\n        self.param = param  ##NEW # <1>\n\n        self.df_trial_results = pd.DataFrame()\n        self.df_trial_results[\"Run Number\"] = [0]\n        self.df_trial_results[\"Arrivals\"] = [0]\n        self.df_trial_results[\"Mean Q Time Recep\"] = [0.0]\n        self.df_trial_results[\"Mean Q Time Nurse\"] = [0.0]\n        self.df_trial_results[\"Mean Q Time Doctor\"] = [0.0]\n        self.df_trial_results.set_index(\"Run Number\", inplace=True)\n\n    def print_trial_results(self):\n        print (\"Trial Results\")\n        print (self.df_trial_results.round(2))\n        print(self.df_trial_results.mean().round(2))\n\n    def run_trial(self):\n        # print(f\"{self.param.number_of_receptionists} receptionists, \" +\n        #      f\"{self.param.number_of_nurses} nurses, \" +\n        #       f\"{self.param.number_of_doctors} doctors\")  ##NEW - no printing # <2>\n        # print(\"\")\n\n        for run in range(self.param.number_of_runs):  ##NEW # <3>\n            random.seed(run)\n\n            my_model = Model(param=self.param, run_number=run)  ##NEW # <3>\n            patient_level_results = my_model.run()\n\n            self.df_trial_results.loc[run] = [\n                len(patient_level_results),\n                my_model.mean_q_time_recep,\n                my_model.mean_q_time_nurse,\n                my_model.mean_q_time_doctor\n                ]\n\n        ##NEW - no printing\n        # self.print_trial_results() # <4>\n```\n:::\n\n\n1. Set `param` as an input to the `Trial`, and made a trial attribute.\n2. Disabled printing.\n3. Replaced all `g` with `self.param`.\n4. Disabled printing.\n\n### The full code\n\nThe full updated code for the model is given below.\n\n:::{.callout-note collapse=\"true\"}\n## Click here to view the code\n\n::: {#97d16cc4 .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport random\nimport simpy\nfrom sim_tools.distributions import Exponential\n\n# Class to store parameter values.\nclass Param:  ##NEW\n    patient_inter = 5\n    mean_reception_time = 2\n    mean_n_consult_time = 6\n    mean_d_consult_time = 20\n    number_of_receptionists = 1\n    number_of_nurses = 1\n    number_of_doctors = 2\n    prob_seeing_doctor = 0.6\n    sim_duration = 600\n    number_of_runs = 100\n\n# Class representing patients coming in to the clinic.\nclass Patient:\n    def __init__(self, p_id):\n        self.id = p_id\n        self.q_time_recep = 0\n        self.q_time_nurse = 0\n        self.q_time_doctor = 0\n\n# Class representing our model of the clinic.\nclass Model:\n    # Constructor to set up the model for a run.  We pass in a run number and\n    # instance of the parameter class when we create a new model.\n    def __init__(self, param, run_number):  ##NEW\n        # Store the passed in parameters\n        self.param = param  ##NEW\n\n        # Create a SimPy environment in which everything will live\n        self.env = simpy.Environment()\n\n        # Create a patient counter (which we'll use as a patient ID)\n        self.patient_counter = 0\n\n        # Create our resources\n        self.receptionist = simpy.Resource(\n            self.env, capacity=self.param.number_of_receptionists)  ##NEW\n        self.nurse = simpy.Resource(\n            self.env, capacity=self.param.number_of_nurses)  ##NEW\n        self.doctor = simpy.Resource(\n            self.env, capacity=self.param.number_of_doctors)  ##NEW\n\n        # Store the passed in run number\n        self.run_number = run_number\n\n        # Create a new Pandas DataFrame that will store some results against\n        # the patient ID (which we'll use as the index).\n        self.results_df = pd.DataFrame()\n        self.results_df[\"Patient ID\"] = [1]\n        self.results_df[\"Q Time Recep\"] = [0.0]\n        self.results_df[\"Time with Recep\"] = [0.0]\n        self.results_df[\"Q Time Nurse\"] = [0.0]\n        self.results_df[\"Time with Nurse\"] = [0.0]\n        self.results_df[\"Q Time Doctor\"] = [0.0]\n        self.results_df[\"Time with Doctor\"] = [0.0]\n        self.results_df.set_index(\"Patient ID\", inplace=True)\n\n        # Create an attribute to store the mean queuing times across this run of\n        # the model\n        self.mean_q_time_recep = 0\n        self.mean_q_time_nurse = 0\n        self.mean_q_time_doctor = 0\n\n        ss = np.random.SeedSequence(self.run_number)\n        seeds = ss.spawn(4)\n        self.patient_inter_arrival_dist = Exponential(\n            mean=self.param.patient_inter,  ##NEW\n            random_seed=seeds[0])\n        self.patient_reception_time_dist = Exponential(\n            mean=self.param.mean_reception_time,  ##NEW\n            random_seed=seeds[1])\n        self.nurse_consult_time_dist = Exponential(\n            mean=self.param.mean_n_consult_time,  ##NEW\n            random_seed=seeds[2])\n        self.doctor_consult_time_dist = Exponential(\n            mean=self.param.mean_d_consult_time,  ##NEW\n            random_seed=seeds[3])\n\n    # A generator function that represents the DES generator for patient\n    # arrivals\n    def generator_patient_arrivals(self):\n        # We use an infinite loop here to keep doing this indefinitely whilst\n        # the simulation runs\n        while True:\n            # Increment the patient counter by 1 (this means our first patient\n            # will have an ID of 1)\n            self.patient_counter += 1\n\n            # Create a new patient - an instance of the Patient Class we\n            # defined above.  Remember, we pass in the ID when creating a\n            # patient - so here we pass the patient counter to use as the ID.\n            p = Patient(self.patient_counter)\n\n            # Tell SimPy to start up the attend_clinic generator function with\n            # this patient (the generator function that will model the\n            # patient's journey through the system)\n            self.env.process(self.attend_clinic(p))\n\n            # Randomly sample the time to the next patient arriving.  Here, we\n            # sample from an exponential distribution (common for inter-arrival\n            # times), and pass in a lambda value of 1 / mean.  The mean\n            # inter-arrival time is stored in the g class.\n            sampled_inter = self.patient_inter_arrival_dist.sample()\n\n            # Freeze this instance of this function in place until the\n            # inter-arrival time we sampled above has elapsed.  Note - time in\n            # SimPy progresses in \"Time Units\", which can represent anything\n            # you like (just make sure you're consistent within the model)\n            yield self.env.timeout(sampled_inter)\n\n    # A generator function that represents the pathway for a patient going\n    # through the clinic.\n    # The patient object is passed in to the generator function so we can\n    # extract information from / record information to it\n    def attend_clinic(self, patient):\n        start_q_recep = self.env.now\n\n        with self.receptionist.request() as req:\n            yield req\n\n            end_q_recep = self.env.now\n\n            patient.q_time_recep = end_q_recep - start_q_recep\n\n            sampled_recep_act_time = self.patient_reception_time_dist.sample()\n\n            self.results_df.at[patient.id, \"Q Time Recep\"] = (\n                 patient.q_time_recep\n            )\n            self.results_df.at[patient.id, \"Time with Recep\"] = (\n                 sampled_recep_act_time\n            )\n\n            yield self.env.timeout(sampled_recep_act_time)\n\n        # Here's where the patient finishes with the receptionist, and starts\n        # queuing for the nurse\n\n        # Record the time the patient started queuing for a nurse\n        start_q_nurse = self.env.now\n\n        # This code says request a nurse resource, and do all of the following\n        # block of code with that nurse resource held in place (and therefore\n        # not usable by another patient)\n        with self.nurse.request() as req:\n            # Freeze the function until the request for a nurse can be met.\n            # The patient is currently queuing.\n            yield req\n\n            # When we get to this bit of code, control has been passed back to\n            # the generator function, and therefore the request for a nurse has\n            # been met.  We now have the nurse, and have stopped queuing, so we\n            # can record the current time as the time we finished queuing.\n            end_q_nurse = self.env.now\n\n            # Calculate the time this patient was queuing for the nurse, and\n            # record it in the patient's attribute for this.\n            patient.q_time_nurse = end_q_nurse - start_q_nurse\n\n            # Now we'll randomly sample the time this patient with the nurse.\n            # Here, we use an Exponential distribution for simplicity, but you\n            # would typically use a Log Normal distribution for a real model\n            # (we'll come back to that).  As with sampling the inter-arrival\n            # times, we grab the mean from the g class, and pass in 1 / mean\n            # as the lambda value.\n            sampled_nurse_act_time = self.nurse_consult_time_dist.sample()\n\n            # Here we'll store the queuing time for the nurse and the sampled\n            # time to spend with the nurse in the results DataFrame against the\n            # ID for this patient.  In real world models, you may not want to\n            # bother storing the sampled activity times - but as this is a\n            # simple model, we'll do it here.\n            # We use a handy property of pandas called .at, which works a bit\n            # like .loc.  .at allows us to access (and therefore change) a\n            # particular cell in our DataFrame by providing the row and column.\n            # Here, we specify the row as the patient ID (the index), and the\n            # column for the value we want to update for that patient.\n            self.results_df.at[patient.id, \"Q Time Nurse\"] = (\n                patient.q_time_nurse)\n            self.results_df.at[patient.id, \"Time with Nurse\"] = (\n                sampled_nurse_act_time)\n\n            # Freeze this function in place for the activity time we sampled\n            # above.  This is the patient spending time with the nurse.\n            yield self.env.timeout(sampled_nurse_act_time)\n\n            # When the time above elapses, the generator function will return\n            # here.  As there's nothing more that we've written, the function\n            # will simply end.  This is a sink.  We could choose to add\n            # something here if we wanted to record something - e.g. a counter\n            # for number of patients that left, recording something about the\n            # patients that left at a particular sink etc.\n\n        # Conditional logic to see if patient goes on to see doctor\n        # We sample from the uniform distribution between 0 and 1.  If the value\n        # is less than the probability of seeing a doctor (stored in g Class)\n        # then we say the patient sees a doctor.\n        # If not, this block of code won't be run and the patient will just\n        # leave the system (we could add in an else if we wanted a branching\n        # path to another activity instead)\n        if random.uniform(0,1) < self.param.prob_seeing_doctor:  ##NEW\n            start_q_doctor = self.env.now\n\n            with self.doctor.request() as req:\n                yield req\n\n                end_q_doctor = self.env.now\n\n                patient.q_time_doctor = end_q_doctor - start_q_doctor\n\n                sampled_doctor_act_time = self.nurse_consult_time_dist.sample()\n\n                self.results_df.at[patient.id, \"Q Time Doctor\"] = (\n                    patient.q_time_doctor\n                )\n                self.results_df.at[patient.id, \"Time with Doctor\"] = (\n                    sampled_doctor_act_time\n                )\n\n                yield self.env.timeout(sampled_doctor_act_time)\n\n    # This method calculates results over a single run.  Here we just calculate\n    # a mean, but in real world models you'd probably want to calculate more.\n    def calculate_run_results(self):\n        # Take the mean of the queuing times across patients in this run of the\n        # model.\n        self.mean_q_time_recep = self.results_df[\"Q Time Recep\"].mean()\n        self.mean_q_time_nurse = self.results_df[\"Q Time Nurse\"].mean()\n        self.mean_q_time_doctor = self.results_df[\"Q Time Doctor\"].mean()\n\n    # The run method starts up the DES entity generators, runs the simulation,\n    # and in turns calls anything we need to generate results for the run\n    def run(self):\n        # Start up our DES entity generators that create new patients.  We've\n        # only got one in this model, but we'd need to do this for each one if\n        # we had multiple generators.\n        self.env.process(self.generator_patient_arrivals())\n\n        # Run the model for the duration specified in g class\n        self.env.run(until=self.param.sim_duration)  ##NEW\n\n        # Now the simulation run has finished, call the method that calculates\n        # run results\n        self.calculate_run_results()\n\n        # Print the run number with the patient-level results from this run of\n        # the model\n        return (self.results_df)\n\n# Class representing a Trial for our simulation - a batch of simulation runs.\nclass Trial:\n    # The constructor sets up a pandas dataframe that will store the key\n    # results from each run against run number, with run number as the index.\n    def  __init__(self, param):  ##NEW\n        # Store the model parameters\n        self.param = param  ##NEW\n\n        self.df_trial_results = pd.DataFrame()\n        self.df_trial_results[\"Run Number\"] = [0]\n        self.df_trial_results[\"Arrivals\"] = [0]\n        self.df_trial_results[\"Mean Q Time Recep\"] = [0.0]\n        self.df_trial_results[\"Mean Q Time Nurse\"] = [0.0]\n        self.df_trial_results[\"Mean Q Time Doctor\"] = [0.0]\n        self.df_trial_results.set_index(\"Run Number\", inplace=True)\n\n    # Method to print out the results from the trial.  In real world models,\n    # you'd likely save them as well as (or instead of) printing them\n    def print_trial_results(self):\n        print (\"Trial Results\")\n        print (self.df_trial_results.round(2))\n        print(self.df_trial_results.mean().round(2))\n\n    # Method to run a trial\n    def run_trial(self):\n        # print(f\"{self.param.number_of_receptionists} receptionists, \" +\n        #      f\"{self.param.number_of_nurses} nurses, \" +\n        #       f\"{self.param.number_of_doctors} doctors\")  ##NEW - no printing\n        # print(\"\")\n        # Run the simulation for the number of runs specified in g class.\n        # For each run, we create a new instance of the Model class and call its\n        # run method, which sets everything else in motion.  Once the run has\n        # completed, we grab out the stored run results (just mean queuing time\n        # here) and store it against the run number in the trial results\n        # dataframe.\n        for run in range(self.param.number_of_runs):  ##NEW\n            random.seed(run)\n\n            my_model = Model(param=self.param, run_number=run)  ##NEW\n            patient_level_results = my_model.run()\n\n            self.df_trial_results.loc[run] = [\n                len(patient_level_results),\n                my_model.mean_q_time_recep,\n                my_model.mean_q_time_nurse,\n                my_model.mean_q_time_doctor\n                ]\n\n        ##NEW - no printing\n        # Once the trial (ie all runs) has completed, print the final results\n        # self.print_trial_results()\n```\n:::\n\n\n:::\n\n## Testing our model\n\nThere are many different ways of categorising tests. We will focus on three types:\n\n* **Functional testing**\n* **Unit testing**\n* **Back testing**\n\n### Functional tests\n\nFunctional tests verify that the system or components perform their intended functionality.\n\nFor example, we expect that the mean wait time for a nurse should decrease if:\n\n* The number of nurses increases.\n* The patient inter-arrival time increases (so there are fewer arrivals).\n* The length of the nurse consultation decreases.\n\nFor simplicity, this test just focuses the nurse waiting times, but this idea can be expanded to other resources and metrics in the model as well.\n\n::: {#7ab9534d .cell execution_count=10}\n``` {.python .cell-code}\nimport pytest\nfrom full_model import Param, Trial\n\n\n@pytest.mark.parametrize('param_name, initial_value, adjusted_value', [\n    ('number_of_nurses', 3, 9),\n    ('patient_inter', 2, 15),\n    ('mean_n_consult_time', 30, 3),\n])\ndef test_waiting_time_utilisation(param_name, initial_value, adjusted_value):\n    \"\"\"\n    Test that adjusting parameters decreases the waiting time and utilisation.\n\n    Arguments:\n        param_name (string):\n            Name of parameter to change in the Param() class.\n        initial_value (float|int):\n            Value with which we expect longer waiting times.\n        adjusted_value (float|int):\n            Value with which we expect shorter waiting time.\n    \"\"\"\n    # Define helper function for the test\n    def helper_param(param_name, value):\n        \"\"\"\n        Helper function to set a specific parameter value, run the model for\n        a single replication and return the results of that run.\n\n        Arguments:\n            param_name (string):\n                Name of the parameter to modify.\n            value (float|int):\n                Value to assign to the parameter.\n\n        Returns:\n            dataframe:\n                Dataframe with the trial-level results.\n        \"\"\"\n        # Create instance of parameter class with default values but one run\n        param = Param()\n        param.number_of_runs = 1\n\n        # Modify specific parameter\n        setattr(param, param_name, value)\n\n        # Run replications and return the results from the run as a series\n        trial = Trial(param)\n        trial.run_trial()\n        return trial.df_trial_results.iloc[0]\n\n    # Run model with initial and adjusted values\n    initial_results = helper_param(param_name, initial_value)\n    adjusted_results = helper_param(param_name, adjusted_value)\n\n    # Check that nurse waiting times from adjusted model are lower\n    initial_wait = initial_results[\"Mean Q Time Nurse\"]\n    adjusted_wait = adjusted_results[\"Mean Q Time Nurse\"]\n    assert initial_wait > adjusted_wait, (\n        f\"Changing '{param_name}' from {initial_value} to {adjusted_value} \" +\n        \"did not decrease waiting time for the nurse as expected: observed \" +\n        f\"waiting times of {initial_wait} and {adjusted_wait}, respectively.\"\n    )\n```\n:::\n\n\nThese tests pass.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n## Test output:\n\n::: {#a262f1a8 .cell execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\n============================= test session starts =============================\nplatform win32 -- Python 3.11.4, pytest-9.0.2, pluggy-1.6.0\nrootdir: C:\\hsma6_des_book\nplugins: anyio-4.8.0\ncollected 3 items\n\ntests\\test_functionaltest.py ...                                         [100%]\n\n============================== 3 passed in 0.34s ==============================\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<ExitCode.OK: 0>\n```\n:::\n:::\n\n\n:::\n\n### Unit tests\n\nUnit tests are a type of functional testing that focuses on individual components (e.g. methods, classes) and tests them in isolation to ensure they work as intended.\n\nFor example, we expect that our model should fail if the number of doctors or the patient inter-arrival time were set to 0. This is tested using `test_zero_inputs`.\n\n::: {#9738abc2 .cell execution_count=12}\n``` {.python .cell-code}\nimport pytest\nfrom full_model import Param, Trial\n\n\n@pytest.mark.parametrize(\"param_name, value\", [\n    (\"number_of_doctors\", 0),\n    (\"patient_inter\", 0)\n])\ndef test_zero_inputs(param_name, value):\n    \"\"\"\n    Check that the model fails when inputs that are zero are used.\n\n    Arguments:\n        param_name (string):\n            Name of parameter to change in the Param() class.\n        value (float|int):\n            Invalid value for parameter.\n    \"\"\"\n    # Create parameter class with an invalid value\n    param = Param()\n    setattr(param, param_name, value)\n\n    # Verify that initialising the model raises an error\n    with pytest.raises(ValueError):\n        Trial(param)\n```\n:::\n\n\nWhen we run the test, we see that both fail.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n## Test output:\n\n::: {#2fc1db25 .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\n============================= test session starts =============================\nplatform win32 -- Python 3.11.4, pytest-9.0.2, pluggy-1.6.0\nrootdir: C:\\hsma6_des_book\nplugins: anyio-4.8.0\ncollected 2 items\n\ntests\\test_unittest.py FF                                                [100%]\n\n================================== FAILURES ===================================\n____________________ test_zero_inputs[number_of_doctors-0] ____________________\n\nparam_name = 'number_of_doctors', value = 0\n\n    @pytest.mark.parametrize(\"param_name, value\", [\n        (\"number_of_doctors\", 0),\n        (\"patient_inter\", 0)\n    ])\n    def test_zero_inputs(param_name, value):\n        \"\"\"\n        Check that the model fails when inputs that are zero are used.\n    \n        Arguments:\n            param_name (string):\n                Name of parameter to change in the Param() class.\n            value (float|int):\n                Invalid value for parameter.\n        \"\"\"\n        # Create parameter class with an invalid value\n        param = Param()\n        setattr(param, param_name, value)\n    \n        # Verify that initialising the model raises an error\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_unittest.py:24: Failed\n______________________ test_zero_inputs[patient_inter-0] ______________________\n\nparam_name = 'patient_inter', value = 0\n\n    @pytest.mark.parametrize(\"param_name, value\", [\n        (\"number_of_doctors\", 0),\n        (\"patient_inter\", 0)\n    ])\n    def test_zero_inputs(param_name, value):\n        \"\"\"\n        Check that the model fails when inputs that are zero are used.\n    \n        Arguments:\n            param_name (string):\n                Name of parameter to change in the Param() class.\n            value (float|int):\n                Invalid value for parameter.\n        \"\"\"\n        # Create parameter class with an invalid value\n        param = Param()\n        setattr(param, param_name, value)\n    \n        # Verify that initialising the model raises an error\n>       with pytest.raises(ValueError):\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_unittest.py:24: Failed\n=========================== short test summary info ===========================\nFAILED tests/test_unittest.py::test_zero_inputs[number_of_doctors-0] - Failed...\nFAILED tests/test_unittest.py::test_zero_inputs[patient_inter-0] - Failed: DI...\n============================== 2 failed in 0.12s ==============================\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<ExitCode.TESTS_FAILED: 1>\n```\n:::\n:::\n\n\n:::\n\nThese tests fail as we do not have an error handling for these values. If we had proceeded to `run_trial()`...\n\n* **Number of doctors = 0:** The model would've stopped, as SimPy has built in functionality requiring that the capacity of resources must be greater than 0, and so it raises a ValueError and stops execution.\n* **Patient inter-arrival time = 0**: The model would have run infinitely, as it would just constantly generating new patients.\n\nTo address this, we could add error handling which raises an error for users if they try to input a value of 0. For example, we could add the following code to our `Model __init__` method:\n\n::: {#63035244 .cell execution_count=14}\n``` {.python .cell-code}\n# Loop through the specified parameters\nfor param_name in [\"sim_duration\", \"patient_inter\"]:\n\n    # Get the value of that parameter by its name\n    param_value = getattr(self.param, param_name)\n\n    # Raise an error if it is 0 or less\n    if param_value <= 0:\n        raise ValueError(\n            f\"Parameter '{param_name}' must be greater than 0, but has been\" +\n            f\"set to {param_value:.3f}.)\n```\n:::\n\n\n### Back tests\n\nBack tests check that the model code produces results consistent with those generated historically/from prior code.\n\nFirst, we'll generate a set of expected results, with a specific set of parameters. Although this may seem unnecessary in this case, as they match our default parameters in our `Param` class, these are still specified to ensure that we are testing on the same parameters, even if defaults change in Param class.\n\n::: {#f7aac901 .cell execution_count=15}\n``` {.python .cell-code}\nparam = Param()\nparam.patient_inter = 5\nparam.mean_reception_time = 2\nparam.mean_n_consult_time = 6\nparam.mean_d_consult_time = 20\nparam.number_of_receptionists = 1\nparam.number_of_nurses = 1\nparam.number_of_doctors = 2\nparam.prob_seeing_doctor = 0.6\nparam.sim_duration = 600\nparam.number_of_runs = 100\n```\n:::\n\n\nWe'll then run the model and save the results to `.csv` files.\n\n::: {#51fbb4c3 .cell execution_count=16}\n``` {.python .cell-code}\n# Run trial\ntrial = Trial(param)\ntrial.run_trial()\n\n# Preview and save results to csv\nprint(trial.df_trial_results.head())\ntrial.df_trial_results.to_csv(\"tests/exp_results/trial.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Arrivals  Mean Q Time Recep  Mean Q Time Nurse  Mean Q Time Doctor\nRun Number                                                                    \n0              115.0           0.784037          84.988116            1.457872\n1              118.0           2.380390          83.806804            0.284548\n2              115.0           1.094317          21.583588            1.580249\n3              126.0           1.733049          54.613322            0.291588\n4              124.0           2.102781          75.413983            0.061135\n```\n:::\n:::\n\n\nIn the test, we'll run the same model parameters, then import and compare against the saved `.csv` file to check for any differences.\n\n::: {#32726fa4 .cell execution_count=17}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport pandas as pd\nfrom full_model import Param, Trial\n\n\ndef test_reproduction():\n    \"\"\"\n    Check that results from particular run of the model match those previously\n    generated using the code.\n    \"\"\"\n    # Define model parameters\n    param = Param()\n    param.patient_inter = 5\n    param.mean_reception_time = 2\n    param.mean_n_consult_time = 6\n    param.mean_d_consult_time = 20\n    param.number_of_receptionists = 1\n    param.number_of_nurses = 1\n    param.number_of_doctors = 2\n    param.prob_seeing_doctor = 0.6\n    param.sim_duration = 600\n    param.number_of_runs = 100\n\n    # Run trial\n    trial = Trial(param)\n    trial.run_trial()\n\n    # Compare the trial results\n    exp_trial = pd.read_csv(\n        Path(__file__).parent.joinpath(\"exp_results/trial.csv\"), index_col=0)\n    pd.testing.assert_frame_equal(trial.df_trial_results, exp_trial)\n```\n:::\n\n\nThis test passes.\n\n:::{.callout-note icon=false collapse=\"true\"}\n\n## Test output:\n\n::: {#357baa4b .cell execution_count=18}\n\n::: {.cell-output .cell-output-stdout}\n```\n============================= test session starts =============================\nplatform win32 -- Python 3.11.4, pytest-9.0.2, pluggy-1.6.0\nrootdir: C:\\hsma6_des_book\nplugins: anyio-4.8.0\ncollected 1 item\n\ntests\\test_backtest.py .                                                 [100%]\n\n============================== 1 passed in 4.53s ==============================\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<ExitCode.OK: 0>\n```\n:::\n:::\n\n\n:::\n\nWe generate the expected results for our backtest in a seperate Python file or Jupyter notebook, rather than within the test itself. We then would generally run tests using the same pre-generated `.csv` files, without regenerating them. However, the test will fail if the model logic is intentionally changed, leading to different results from the same parameters. In that case, if we are certain that these changes are the reason for differing results, we should re-run the Python file or notebook to regenerate the `.csv`. It is crucial to exercise caution when doing this, to avoid unintentionally overwriting correct expected results.\n\n### Further testing examples\n\nFor more inspiration, check out the [pythonhealthdatascience/pydesrap_mms](https://github.com/pythonhealthdatascience/pydesrap_mms). Examples of other tests it includes are:\n\n* Functional tests for the impact of high demand on utilisation.\n* Functional tests checking for expected decreases in the number of arrivals.\n* Functional tests for an interval auditor.\n* Functional tests for parallel execution.\n* Functional tests for a warm-up period.\n* Unit tests for a logging class.\n* Unit tests for a modified parameter class which has functionality designed to prevent the addition of new attributes.\n\n",
    "supporting": [
      "tests_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}